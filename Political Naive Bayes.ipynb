{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes on Political Text\n",
    "## Gonzalo Blazquez\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns\n",
    "from nltk.corpus import stopwords\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "\n",
    "# Stopwords\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    tokens2 = [word for word in tokens if word not in sw]\n",
    "    return(tokens2)\n",
    " \n",
    "def remove_punctuation(text, punct_set=punctuation) : \n",
    "    return(''.join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) :     \n",
    "    tokens = text.split()\n",
    "    return(tokens)\n",
    "\n",
    "def lower_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def isalpha_tokens(tokens):\n",
    "    return [token for token in tokens if token.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the conventions data base file\n",
    "convention_db = sqlite3.connect(\"/users/Gonzalo B/Downloads/AppliedTextMining/Module4/Assign4/data/2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'party', 'TEXT', 0, None, 0),\n",
       " (1, 'night', 'INTEGER', 0, None, 0),\n",
       " (2, 'speaker', 'TEXT', 0, None, 0),\n",
       " (3, 'speaker_count', 'INTEGER', 0, None, 0),\n",
       " (4, 'time', 'TEXT', 0, None, 0),\n",
       " (5, 'text', 'TEXT', 0, None, 0),\n",
       " (6, 'text_len', 'TEXT', 0, None, 0),\n",
       " (7, 'file', 'TEXT', 0, None, 0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = convention_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "tables\n",
    "columns = convention_cur.execute(f\"PRAGMA table_info({'conventions'});\").fetchall()\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" exercise. First, we'll pull in the text \n",
    "for each party and prepare it for use in Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill the above list up with items that are themselves lists. The \n",
    "# sublists will have two elements. The first element in the sublist\n",
    "# should be the speech in a single string. The second element\n",
    "# of the sublist should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT text as speech, party as party\n",
    "                            FROM conventions\n",
    "                            WHERE party != 'Other';\n",
    "                            ''')\n",
    "\n",
    "for row in query_results :\n",
    "    # store the results in convention_data\n",
    "    speech, party = row\n",
    "    convention_data.append([speech, party])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a best practice to close up your DB connection when you're done\n",
    "convention_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['It’s too late, Joe. In the face of left wing anarchy and mayhem in Minneapolis, Chicago, and other cities, Joe Biden’s campaign did not condemn it. They donated to it. At least 13 members of Joe Biden’s campaign staff donated to a fund to bail out vandals, arsenals, anarchists, looters, and rioters from jail. Here tonight is the grieving family of retired Police Captain David Dorn, a 38-year veteran of the St. Louis Police Department, a great man and a highly respected man by all. In June, Captain Dorn was shot and killed as he tried to protect a store from rioters and looters, or as the Democrats would call them, peaceful protestors.',\n",
       "  'Republican'],\n",
       " ['At the time the police considered domestic violence, something that was not a crime in the home, it’s a private matter. And so women were responsible for their own injuries.',\n",
       "  'Democratic'],\n",
       " ['This year’s election is very important. Probably be the most important election we’ve had in years. I recommend strongly based on the division in this country created by our current president, Donald Trump, we need to put somebody else in the White House that’s going to bring us together. Now, let me just explain something. I’ve been a longstanding Republican for a long time, and I’m telling you, you got to vote for Joe Biden. You have to. I don’t think we can deal with the type of person we have in the White House any longer. So it’s up to you, America, and me, because in this election, I’m voting for Joe. I’m sure, I’m absolutely sure he’s going to help us bring this country together once again.',\n",
       "  'Democratic'],\n",
       " ['And when he called me and said, “I want you to represent my entire campaign,” I became the first black woman to represent a Republican presidential campaign, winning presidential campaign in United States history.',\n",
       "  'Republican'],\n",
       " ['I’m Sean Parnell, and it is an honor to be here. In 2006, the army sent me to Afghanistan as a young platoon leader placed in command of Americans from every corner of our planet. Our platoon reflected the diversity of our nation, every race, creed, and religion. Despite those differences, we were bound together as brothers from the same American family. On June 10th, 2006, our platoon was attacked just after dawn. Outnumbered, 10 to one, we endured mortar and machine gunfire as hundreds of Taliban charged us from three sides. We had 24 men that day. Wave after wave of Taliban advanced up the hill. I was wounded three times in the fighting. Nearly all of my platoon was wounded within the first minute, but the enemy kept coming. We fought to our last rounds of ammunition and when it was over, we held the hill.',\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll be useful for us to have a large sample size than 2024 affords, since those speeches tend to be long and contiguous. Let's make a new list-of-lists called `conv_sent_data`. Instead of each first entry in the sublists being an entire speech, make each first entry just a sentence from the speech. Feel free to use NLTK's `sent_tokenize` [function](https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024 instructions\n",
    "#conv_sent_data = []\n",
    "\n",
    "#for speech, party in convention_data :\n",
    " #   pass # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's look at some random entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.choices(conv_sent_data,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for our final cleaning before modeling. Go through `conv_sent_data` and take the following steps: \n",
    "\n",
    "1. Tokenize on whitespace\n",
    "1. Remove punctuation\n",
    "1. Remove tokens that fail the `isalpha` test\n",
    "1. Remove stopwords\n",
    "1. Casefold to lowercase\n",
    "1. Join the remaining tokens into a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('going invite join faith mine pray agreement lord come ask spirit peace come hurting communities wisconsin tonight pray healing comfort jacob blake family pray protection put lives way bring safety security streets pray truth justice heart decisions made leaders people seek reconciliation thank lord goodness lives blessing nation guiding hand every person calls united states america home lord thank gone us sowed seeds sacrifice freedom prosperity peace gave lives could live achieve american dream',\n",
       "  'Republican'),\n",
       " ('called duty america long history enemies fear us americans fight good know gives us strength heroes trusted equipped freedom prevails defeat isis result america believing heroes president backs rebuilding military needed finish mission cowering iranian regime restoration deterrence lost result america believing might heroism relegated battlefield every single day see know look nurse volunteers back back shifts caring covid patients feels duty parent relearn algebra way letting kid fall behind schools closed',\n",
       "  'Republican'),\n",
       " ('pledge american workers definitely means lot today truly believe kids going look one day tremendous could guess',\n",
       "  'Republican'),\n",
       " ('iowa', 'Republican'),\n",
       " ('john elected senate republican arizona found opposing sides', 'Democratic')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_conv_sent_data = [] # list of tuples (speech, party), with sentence cleaned\n",
    "\n",
    "for idx, (speech, party) in enumerate(convention_data):\n",
    "    \n",
    "    speech_punct = remove_punctuation(speech, punctuation) # before token becuase it works with text\n",
    "    tokens = tokenize(speech_punct)\n",
    "    tokens = isalpha_tokens(tokens)\n",
    "    tokens = lower_tokens(tokens) # before sw to remove 'The'\n",
    "    tokens = remove_stop(tokens)\n",
    "    cleaned_sentence = join_tokens(tokens)\n",
    "    \n",
    "    # store the results\n",
    "    clean_conv_sent_data.append((cleaned_sentence, party))\n",
    "\n",
    "random.choices(clean_conv_sent_data,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, let's make our function to turn these into features. First we need to build our list of candidate words. I started my exploration at a cutoff of 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2255 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in clean_conv_sent_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a word cutoff of 5, we have 2255 as features in the model.\n",
    "\n",
    "With a word cutoff of 10, we have 1352 as features in the model.\n",
    "\n",
    "With a word cutoff of 20, we have 754 as features in the model.\n",
    "\n",
    "With a word cutoff of 30, we have 500 as features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    ret_dict = dict()\n",
    "    words=tokenize(text)\n",
    "\n",
    "    for word in words:\n",
    "        if word in fw:\n",
    "            ret_dict[word] = True \n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"obama was the president\",feature_words)==\n",
    "       {'obama':True,'president':True})\n",
    "assert(conv_features(\"some people in america are citizens\",feature_words)==\n",
    "                     {'people':True,'america':True,\"citizens\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.498\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count / Accuracy\n",
    "\n",
    "10  /  0.482\n",
    "\n",
    "5   /  0.498     \n",
    "\n",
    "30  /  0.474    \n",
    "\n",
    "20  /  0.468  \n",
    "\n",
    "1   /  0.492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             enforcement = True           Republ : Democr =     27.5 : 1.0\n",
      "                   votes = True           Democr : Republ =     21.6 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.3 : 1.0\n",
      "                 destroy = True           Republ : Democr =     17.1 : 1.0\n",
      "                supports = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     15.9 : 1.0\n",
      "                preserve = True           Republ : Democr =     15.1 : 1.0\n",
      "                  signed = True           Republ : Democr =     15.1 : 1.0\n",
      "              appreciate = True           Republ : Democr =     14.0 : 1.0\n",
      "                freedoms = True           Republ : Democr =     14.0 : 1.0\n",
      "                 private = True           Republ : Democr =     11.9 : 1.0\n",
      "                  defund = True           Republ : Democr =     10.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.3 : 1.0\n",
      "                 special = True           Republ : Democr =     10.3 : 1.0\n",
      "                   trade = True           Republ : Democr =     10.0 : 1.0\n",
      "                everyday = True           Republ : Democr =      9.9 : 1.0\n",
      "                   local = True           Republ : Democr =      9.9 : 1.0\n",
      "                 allowed = True           Republ : Democr =      9.7 : 1.0\n",
      "                   elect = True           Democr : Republ =      9.6 : 1.0\n",
      "                   moved = True           Republ : Democr =      9.0 : 1.0\n",
      "                   bless = True           Republ : Democr =      9.0 : 1.0\n",
      "                    land = True           Republ : Democr =      8.9 : 1.0\n",
      "                  agenda = True           Republ : Democr =      8.8 : 1.0\n",
      "               countries = True           Republ : Democr =      8.8 : 1.0\n",
      "                   crime = True           Republ : Democr =      8.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "After testing different thresholds for the word cutoff, the best number is 5 words to be counted at least. The resulting accuracy for that amount is only 0.498, thus for a binary result, is like flipping a coin. The informative features gives more information about each category, specifically the words that are more common for Republicans and Democrats. For example, words like 'enforcement', 'destroy', and 'supports' are related to Republicans, and 'votes', 'climate', and 'elect' to Democrats. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"/users/Gonzalo B/Downloads/AppliedTextMining/Module4/Assign4/data/congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "\n",
    "for row in results :\n",
    "    # store the results in tweet_data\n",
    "    candidate, party, tweet = row\n",
    "    tweet_data.append([tweet, party])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: b'Earlier today, I spoke on the House Floor abt protecting health care for women and praised @PPmarmonte for their work on the Central Coast. https://t.co/WqgTRzT7VV'\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b'Go Tribe! #RallyTogether https://t.co/0NXutFL9L5'\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b\"Apparently, Trump thinks it's just too easy for students overwhelmed by the crushing burden of debt to pay off student loans #TrumpBudget https://t.co/ckYQO5T0Qh\"\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b'We\\xe2\\x80\\x99re grateful for our first responders, our rescue personnel, our firefighters, our police, and volunteers who have been working tirelessly to keep people safe, provide much-needed help, while putting their own lives on the line.\\n\\nhttps://t.co/eZPv0vMIz3'\n",
      "Actual party is Republican and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b'Let\\xe2\\x80\\x99s make it even Greater !! #KAG \\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8 https://t.co/y9qoZD5L2z'\n",
      "Actual party is Republican and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b\"We have about 1hr until the @cavs tie up the series 2-2. I'm #ALLin216 @RepBarbaraLee you scared? #roadtovictory\"\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b'Congrats to @belliottsd on his new gig at SD City Hall. We are glad you will continue to serve\\xe2\\x80\\xa6 https://t.co/fkvMw3cqdI'\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b'We are really close, we have over $3500 raised toward the match right now. Whoot!! (That\\xe2\\x80\\x99s $7000 for the non-math majors in the room \\xf0\\x9f\\x98\\x82). Help us get there https://t.co/Tu34C472sD https://t.co/QsdQkYpsmC'\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b'Today, the comment period for @POTUS\\xe2\\x80\\x99s plan to expand offshore drilling opened to the public. You have 60 days (until March 9) to share why you oppose the proposed program directly with the Trump Administration. Comments can be made by email or mail. https://t.co/BaaYMeJxQn'\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: b'Celebrated @ICSEastLA\\xe2\\x80\\x99s 22 years of Eastside commitment &amp; saluted community leaders at last night\\xe2\\x80\\x99s awards dinner! https://t.co/7V7gH8giVB'\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample :\n",
    "    estimated_party = classifier.classify(conv_features(tweet, feature_words)) # you need to fill this in.\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = classifier.classify(conv_features(tweet, feature_words))\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 0, 'Democratic': 4278}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 0, 'Democratic': 5724})})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "With this second test we can confirm that the classifier is not working as it should, it is predicting 'Democratic' for all of the 'Republican' values, and 'Democratic' for all the 'Democratic' values, thus it is always predicting 'Democratic'. This behavior is not useful for our goal of classifying tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
